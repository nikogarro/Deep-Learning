{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikogarro/Deep-Learning/blob/main/ud187-tensorflow/10-4-nlp-optimizing-the-text-generation-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Optimizing the Text Generation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c04_nlp_optimizing_the_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c04_nlp_optimizing_the_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCxhW3mtLmfb"
      },
      "source": [
        "You've already done some amazing work with generating new songs, but so far we've seen some issues with repetition and a fair amount of incoherence. By using more data and further tweaking the model, you'll be able to get improved results. We'll once again use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics) here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "ac9b7bc8-6a89-4ef2-f88f-c15a6f91cd3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-31 11:24:16--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.31.113, 74.125.31.100, 74.125.31.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.31.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n8op86ccet2shsa378mhb1fje4ofo9ev/1675164225000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=38d1e522-8b74-44ad-96aa-a0bd2b3073b3 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-01-31 11:24:19--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n8op86ccet2shsa378mhb1fje4ofo9ev/1675164225000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=38d1e522-8b74-44ad-96aa-a0bd2b3073b3\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 74.125.26.132, 2607:f8b0:400c:c04::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|74.125.26.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M  78.3MB/s    in 0.9s    \n",
            "\n",
            "2023-01-31 11:24:20 (78.3 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz9x-7dWihxx"
      },
      "source": [
        "## 250 Songs\n",
        "\n",
        "Now we've seen a model trained on just a small sample of songs, and how this often leads to repetition as you get further along in trying to generate new text. Let's switch to using the 250 songs instead, and see if our output improves. This will actually be nearly 10K lines of lyrics, which should be sufficient.\n",
        "\n",
        "Note that we won't use the full dataset here as it will take up quite a bit of RAM and processing time, but you're welcome to try doing so on your own later. If interested, you'll likely want to use only some of the more common words for the Tokenizer, which will help shrink processing time and memory needed (or else you'd have an output array hundreds of thousands of words long)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWbMN_19jfRT"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LRmPPJegovBe"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kIGedF3XjHj4",
        "outputId": "412a1d1a-8d84-46ef-a5c8-5e316a09edc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
            "<ipython-input-4-fbdddccf8583>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
            "<ipython-input-4-fbdddccf8583>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[field] = dataset[field].str.lower()\n"
          ]
        }
      ],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "# Read the dataset from csv - this time with 250 songs\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:250]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus, num_words=2000)\n",
        "total_words = tokenizer.num_words\n",
        "\n",
        "# There should be a lot more words now\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quoDmw_FkNBA"
      },
      "source": [
        "### Create Sequences and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kkLAf3HmkPSo"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cECbqT-blMk-"
      },
      "source": [
        "### Train a (Better) Text Generation Model\n",
        "\n",
        "With more data, we'll cut off after 100 epochs to avoid keeping you here all day. You'll also want to change your runtime type to GPU if you haven't already (you'll need to re-run the above cells if you change runtimes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7nHOp6uWlP_P",
        "outputId": "7a6a4e48-223c-482e-9c42-c7edbbad62a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1480/1480 [==============================] - 21s 7ms/step - loss: 5.9797 - accuracy: 0.0469\n",
            "Epoch 2/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 5.6867 - accuracy: 0.0514\n",
            "Epoch 3/100\n",
            "1480/1480 [==============================] - 11s 8ms/step - loss: 5.5060 - accuracy: 0.0637\n",
            "Epoch 4/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 5.3485 - accuracy: 0.0813\n",
            "Epoch 5/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 5.2106 - accuracy: 0.1019\n",
            "Epoch 6/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 5.0675 - accuracy: 0.1213\n",
            "Epoch 7/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.9329 - accuracy: 0.1350\n",
            "Epoch 8/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.8104 - accuracy: 0.1475\n",
            "Epoch 9/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.6989 - accuracy: 0.1602\n",
            "Epoch 10/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.5982 - accuracy: 0.1691\n",
            "Epoch 11/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.5066 - accuracy: 0.1776\n",
            "Epoch 12/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.4207 - accuracy: 0.1863\n",
            "Epoch 13/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.3376 - accuracy: 0.1954\n",
            "Epoch 14/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.2607 - accuracy: 0.2038\n",
            "Epoch 15/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.1906 - accuracy: 0.2101\n",
            "Epoch 16/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.1246 - accuracy: 0.2173\n",
            "Epoch 17/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.0630 - accuracy: 0.2239\n",
            "Epoch 18/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 4.0065 - accuracy: 0.2332\n",
            "Epoch 19/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.9513 - accuracy: 0.2406\n",
            "Epoch 20/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.9006 - accuracy: 0.2468\n",
            "Epoch 21/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.8542 - accuracy: 0.2539\n",
            "Epoch 22/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.8095 - accuracy: 0.2600\n",
            "Epoch 23/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.7686 - accuracy: 0.2675\n",
            "Epoch 24/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.7264 - accuracy: 0.2727\n",
            "Epoch 25/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.6870 - accuracy: 0.2796\n",
            "Epoch 26/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.6496 - accuracy: 0.2847\n",
            "Epoch 27/100\n",
            "1480/1480 [==============================] - 11s 8ms/step - loss: 3.6113 - accuracy: 0.2894\n",
            "Epoch 28/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.5740 - accuracy: 0.2957\n",
            "Epoch 29/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.5405 - accuracy: 0.2989\n",
            "Epoch 30/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.5091 - accuracy: 0.3062\n",
            "Epoch 31/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.4812 - accuracy: 0.3088\n",
            "Epoch 32/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.4490 - accuracy: 0.3140\n",
            "Epoch 33/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.4228 - accuracy: 0.3175\n",
            "Epoch 34/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.3987 - accuracy: 0.3220\n",
            "Epoch 35/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.3762 - accuracy: 0.3245\n",
            "Epoch 36/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.3493 - accuracy: 0.3295\n",
            "Epoch 37/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.3258 - accuracy: 0.3329\n",
            "Epoch 38/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.3061 - accuracy: 0.3341\n",
            "Epoch 39/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.2856 - accuracy: 0.3380\n",
            "Epoch 40/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.2622 - accuracy: 0.3429\n",
            "Epoch 41/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.2435 - accuracy: 0.3440\n",
            "Epoch 42/100\n",
            "1480/1480 [==============================] - 11s 8ms/step - loss: 3.2247 - accuracy: 0.3491\n",
            "Epoch 43/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.2089 - accuracy: 0.3490\n",
            "Epoch 44/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.1852 - accuracy: 0.3543\n",
            "Epoch 45/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.1692 - accuracy: 0.3566\n",
            "Epoch 46/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.1599 - accuracy: 0.3585\n",
            "Epoch 47/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.1414 - accuracy: 0.3608\n",
            "Epoch 48/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.1206 - accuracy: 0.3654\n",
            "Epoch 49/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.1063 - accuracy: 0.3662\n",
            "Epoch 50/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.0969 - accuracy: 0.3672\n",
            "Epoch 51/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.0763 - accuracy: 0.3724\n",
            "Epoch 52/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.0660 - accuracy: 0.3736\n",
            "Epoch 53/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.0485 - accuracy: 0.3762\n",
            "Epoch 54/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.0434 - accuracy: 0.3778\n",
            "Epoch 55/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 3.0195 - accuracy: 0.3817\n",
            "Epoch 56/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 3.0082 - accuracy: 0.3843\n",
            "Epoch 57/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.9974 - accuracy: 0.3862\n",
            "Epoch 58/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.9858 - accuracy: 0.3889\n",
            "Epoch 59/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.9697 - accuracy: 0.3904\n",
            "Epoch 60/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.9630 - accuracy: 0.3918\n",
            "Epoch 61/100\n",
            "1480/1480 [==============================] - 11s 8ms/step - loss: 2.9449 - accuracy: 0.3950\n",
            "Epoch 62/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.9391 - accuracy: 0.3945\n",
            "Epoch 63/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.9248 - accuracy: 0.3985\n",
            "Epoch 64/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.9113 - accuracy: 0.4008\n",
            "Epoch 65/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.8977 - accuracy: 0.4024\n",
            "Epoch 66/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.8937 - accuracy: 0.4030\n",
            "Epoch 67/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.8812 - accuracy: 0.4048\n",
            "Epoch 68/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 2.8668 - accuracy: 0.4090\n",
            "Epoch 69/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.8570 - accuracy: 0.4095\n",
            "Epoch 70/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.8457 - accuracy: 0.4100\n",
            "Epoch 71/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.8353 - accuracy: 0.4126\n",
            "Epoch 72/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.8210 - accuracy: 0.4154\n",
            "Epoch 73/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.8126 - accuracy: 0.4171\n",
            "Epoch 74/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 2.8064 - accuracy: 0.4187\n",
            "Epoch 75/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.7910 - accuracy: 0.4207\n",
            "Epoch 76/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.7830 - accuracy: 0.4216\n",
            "Epoch 77/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.7706 - accuracy: 0.4238\n",
            "Epoch 78/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.7625 - accuracy: 0.4236\n",
            "Epoch 79/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.7525 - accuracy: 0.4274\n",
            "Epoch 80/100\n",
            "1480/1480 [==============================] - 11s 8ms/step - loss: 2.7455 - accuracy: 0.4290\n",
            "Epoch 81/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.7327 - accuracy: 0.4302\n",
            "Epoch 82/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.7226 - accuracy: 0.4313\n",
            "Epoch 83/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.7150 - accuracy: 0.4341\n",
            "Epoch 84/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.7162 - accuracy: 0.4351\n",
            "Epoch 85/100\n",
            "1480/1480 [==============================] - 11s 8ms/step - loss: 2.7046 - accuracy: 0.4353\n",
            "Epoch 86/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.6999 - accuracy: 0.4356\n",
            "Epoch 87/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.6833 - accuracy: 0.4384\n",
            "Epoch 88/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.6794 - accuracy: 0.4385\n",
            "Epoch 89/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.6729 - accuracy: 0.4390\n",
            "Epoch 90/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.6596 - accuracy: 0.4435\n",
            "Epoch 91/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.6656 - accuracy: 0.4410\n",
            "Epoch 92/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.6489 - accuracy: 0.4450\n",
            "Epoch 93/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.6445 - accuracy: 0.4463\n",
            "Epoch 94/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.6287 - accuracy: 0.4476\n",
            "Epoch 95/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 2.6207 - accuracy: 0.4509\n",
            "Epoch 96/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.6166 - accuracy: 0.4510\n",
            "Epoch 97/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.6099 - accuracy: 0.4524\n",
            "Epoch 98/100\n",
            "1480/1480 [==============================] - 10s 7ms/step - loss: 2.5992 - accuracy: 0.4515\n",
            "Epoch 99/100\n",
            "1480/1480 [==============================] - 11s 7ms/step - loss: 2.5913 - accuracy: 0.4532\n",
            "Epoch 100/100\n",
            "1480/1480 [==============================] - 11s 8ms/step - loss: 2.5937 - accuracy: 0.4550\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgvIz20nlQcq"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rOqmmarvlSLh",
        "outputId": "4d3e7228-4906-47ed-b161-f8f22bd27fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISLZZGlQlSxh"
      },
      "source": [
        "### Generate better lyrics!\n",
        "\n",
        "This time around, we should be able to get a more interesting output with less repetition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "P96oVMk3lU7y",
        "outputId": "2ca55372-6ae1-4953-ae4a-a902e9fbd715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 808ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "im feeling chills to be in the local police car car darling were movin hour bitch slack blue trip happiest extra explain floor meet explain floor picture sun lights rhythm of prolonged friend west in fire of fire one friend seine falling seine floor book test present he spell lala of up light of the street me stinky up wellplanned of dreamworld sound day of twelve attraction day of twelve wide devil rotten of day n happiest december december way grand swimming sigh present car darling dont judge slack through tell danced now babe of love caviar devotions bound burnin lazy hour i\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgJKV8_oRU9"
      },
      "source": [
        "### Varying the Possible Outputs\n",
        "\n",
        "In running the above, you may notice that the same seed text will generate similar outputs. This is because the code is currently always choosing the top predicted class as the next word. What if you wanted more variance in the output? \n",
        "\n",
        "Switching from `model.predict_classes` to `model.predict_proba` will get us all of the class probabilities. We can combine this with `np.random.choice` to select a given predicted output based on a probability, thereby giving a bit more randomness to our outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lZe9gaJeoGVP",
        "outputId": "8306a81f-f04a-4818-8a27-53021b79e012",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "17\n"
          ]
        }
      ],
      "source": [
        "# Test the method with just the first word after the seed text\n",
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "predicted_probs = model.predict(token_list)[0]\n",
        "predicted = np.random.choice([x for x in range(len(predicted_probs))], \n",
        "                             p=predicted_probs)\n",
        "# Running this cell multiple times should get you some variance in output\n",
        "print(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ee7WKgRGrJy1",
        "outputId": "70b9f8a9-940f-4203-e7cb-56108a88b1b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "im feeling chills to be hard to hug him into myself fire by her mornin of full memories baby happiest promises success like rain fuck try bluest limousine kill noise smells noise kisses trip happiest lonely world resist rough lights rhythm past absentminded result park life result laid complain city fuse now do my babys is dumb on a fantasy who used spell you tell it to the more wine yeah yeah in you mind ah i got to give to stay away cause im so free and way to you to do to me to your world off familiar river but you\n"
          ]
        }
      ],
      "source": [
        "# Use this process for the full output generation\n",
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predicted_probs = model.predict(token_list)[0]\n",
        "  predicted = np.random.choice([x for x in range(len(predicted_probs))],\n",
        "                               p=predicted_probs)\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c04_nlp_optimizing_the_text_generation_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}