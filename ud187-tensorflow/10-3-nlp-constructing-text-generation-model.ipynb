{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikogarro/Deep-Learning/blob/main/ud187-tensorflow/10-3-nlp-constructing-text-generation-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "c33cb7cd-7a26-416e-cf23-7e09c47be0cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-31 10:21:19--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.194.138, 172.217.194.101, 172.217.194.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.194.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/emhosv1c70j6ggv48nf72is0o5l5tv2h/1675160475000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=368e17d9-45a8-4717-bbeb-de7993fc4e29 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-01-31 10:21:23--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/emhosv1c70j6ggv48nf72is0o5l5tv2h/1675160475000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=368e17d9-45a8-4717-bbeb-de7993fc4e29\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 74.125.68.132, 2404:6800:4003:c02::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|74.125.68.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M  71.9MB/s    in 1.0s    \n",
            "\n",
            "2023-01-31 10:21:25 (71.9 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "2656892a-c983-42aa-c7b9-e4a485fadb82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "80238f47-4999-47d9-be5a-f858c98d77c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "9c4afb6e-75e8-43f2-8845-ff2b7992cfa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 11s 7ms/step - loss: 5.9967 - accuracy: 0.0293\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.4368 - accuracy: 0.0328\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.3634 - accuracy: 0.0409\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 5.3115 - accuracy: 0.0399\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.2395 - accuracy: 0.0394\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.1670 - accuracy: 0.0429\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.1005 - accuracy: 0.0424\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.0362 - accuracy: 0.0439\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.9634 - accuracy: 0.0585\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.8864 - accuracy: 0.0671\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.8017 - accuracy: 0.0827\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.7102 - accuracy: 0.0898\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.6197 - accuracy: 0.0918\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.5282 - accuracy: 0.1145\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4346 - accuracy: 0.1120\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3494 - accuracy: 0.1322\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.2548 - accuracy: 0.1443\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1625 - accuracy: 0.1680\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.0689 - accuracy: 0.1741\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.9709 - accuracy: 0.1887\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8935 - accuracy: 0.2079\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.8195 - accuracy: 0.2170\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7391 - accuracy: 0.2467\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.6648 - accuracy: 0.2523\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5813 - accuracy: 0.2639\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5118 - accuracy: 0.2886\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4219 - accuracy: 0.3083\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.3532 - accuracy: 0.3169\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.2832 - accuracy: 0.3315\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.2262 - accuracy: 0.3491\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1763 - accuracy: 0.3431\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.1311 - accuracy: 0.3572\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0553 - accuracy: 0.3693\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9906 - accuracy: 0.3930\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9295 - accuracy: 0.4031\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8802 - accuracy: 0.4188\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8257 - accuracy: 0.4243\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7603 - accuracy: 0.4359\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.7058 - accuracy: 0.4440\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.6546 - accuracy: 0.4556\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.6171 - accuracy: 0.4647\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.5563 - accuracy: 0.4768\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.5086 - accuracy: 0.4874\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4787 - accuracy: 0.4975\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.4533 - accuracy: 0.4995\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.3897 - accuracy: 0.5081\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3407 - accuracy: 0.5217\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2901 - accuracy: 0.5303\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2544 - accuracy: 0.5343\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2077 - accuracy: 0.5499\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1646 - accuracy: 0.5560\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1374 - accuracy: 0.5626\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0865 - accuracy: 0.5827\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.0504 - accuracy: 0.5848\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.0283 - accuracy: 0.5878\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9924 - accuracy: 0.5969\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9544 - accuracy: 0.6019\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9159 - accuracy: 0.6130\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8867 - accuracy: 0.6256\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8470 - accuracy: 0.6297\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.8134 - accuracy: 0.6398\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7930 - accuracy: 0.6423\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7619 - accuracy: 0.6554\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7297 - accuracy: 0.6574\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.7247 - accuracy: 0.6564\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6636 - accuracy: 0.6695\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6319 - accuracy: 0.6801\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5954 - accuracy: 0.6842\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.5720 - accuracy: 0.6912\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.5409 - accuracy: 0.6978\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.5199 - accuracy: 0.7013\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4906 - accuracy: 0.7190\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4626 - accuracy: 0.7134\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4323 - accuracy: 0.7255\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.4107 - accuracy: 0.7275\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3846 - accuracy: 0.7361\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3653 - accuracy: 0.7366\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3503 - accuracy: 0.7402\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3229 - accuracy: 0.7447\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3023 - accuracy: 0.7487\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3038 - accuracy: 0.7503\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2875 - accuracy: 0.7518\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2553 - accuracy: 0.7533\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2454 - accuracy: 0.7548\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2305 - accuracy: 0.7533\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2060 - accuracy: 0.7664\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1839 - accuracy: 0.7669\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1558 - accuracy: 0.7780\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1335 - accuracy: 0.7820\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1225 - accuracy: 0.7830\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1149 - accuracy: 0.7815\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1148 - accuracy: 0.7785\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0849 - accuracy: 0.7871\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0655 - accuracy: 0.7926\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0411 - accuracy: 0.7941\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0170 - accuracy: 0.8027\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0006 - accuracy: 0.8032\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9830 - accuracy: 0.8073\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9858 - accuracy: 0.8078\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9756 - accuracy: 0.8088\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9604 - accuracy: 0.8108\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9434 - accuracy: 0.8128\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9330 - accuracy: 0.8163\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9318 - accuracy: 0.8148\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9319 - accuracy: 0.8174\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8997 - accuracy: 0.8204\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8850 - accuracy: 0.8249\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8821 - accuracy: 0.8214\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8833 - accuracy: 0.8169\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8615 - accuracy: 0.8269\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8453 - accuracy: 0.8264\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8204 - accuracy: 0.8345\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8023 - accuracy: 0.8421\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7989 - accuracy: 0.8335\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7973 - accuracy: 0.8385\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7971 - accuracy: 0.8345\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7736 - accuracy: 0.8406\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7678 - accuracy: 0.8421\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7630 - accuracy: 0.8380\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7489 - accuracy: 0.8365\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7435 - accuracy: 0.8446\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7416 - accuracy: 0.8466\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7258 - accuracy: 0.8441\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7134 - accuracy: 0.8512\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7018 - accuracy: 0.8537\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.8537\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6979 - accuracy: 0.8517\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6874 - accuracy: 0.8527\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6914 - accuracy: 0.8512\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6714 - accuracy: 0.8567\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6583 - accuracy: 0.8628\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6439 - accuracy: 0.8653\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6345 - accuracy: 0.8602\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6300 - accuracy: 0.8648\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6224 - accuracy: 0.8618\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6130 - accuracy: 0.8678\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.6084 - accuracy: 0.8648\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.6091 - accuracy: 0.8623\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5983 - accuracy: 0.8678\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5941 - accuracy: 0.8703\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6032 - accuracy: 0.8638\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6409 - accuracy: 0.8537\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6282 - accuracy: 0.8567\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6278 - accuracy: 0.8557\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5968 - accuracy: 0.8653\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5966 - accuracy: 0.8567\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5793 - accuracy: 0.8668\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5594 - accuracy: 0.8693\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5430 - accuracy: 0.8703\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5528 - accuracy: 0.8729\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5466 - accuracy: 0.8653\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5366 - accuracy: 0.8754\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5327 - accuracy: 0.8764\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5274 - accuracy: 0.8769\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5148 - accuracy: 0.8764\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5083 - accuracy: 0.8769\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5011 - accuracy: 0.8809\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5100 - accuracy: 0.8769\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4992 - accuracy: 0.8779\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4930 - accuracy: 0.8814\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4871 - accuracy: 0.8850\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4874 - accuracy: 0.8819\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4806 - accuracy: 0.8880\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4764 - accuracy: 0.8850\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4707 - accuracy: 0.8855\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4698 - accuracy: 0.8860\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4714 - accuracy: 0.8865\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4691 - accuracy: 0.8855\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5373 - accuracy: 0.8618\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4899 - accuracy: 0.8804\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4692 - accuracy: 0.8855\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4552 - accuracy: 0.8819\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4484 - accuracy: 0.8905\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4464 - accuracy: 0.8885\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4391 - accuracy: 0.8835\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4353 - accuracy: 0.8900\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4300 - accuracy: 0.8905\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4352 - accuracy: 0.8870\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4287 - accuracy: 0.8920\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4231 - accuracy: 0.8900\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4166 - accuracy: 0.8905\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4098 - accuracy: 0.8935\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4098 - accuracy: 0.8905\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4090 - accuracy: 0.8910\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4096 - accuracy: 0.8870\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3998 - accuracy: 0.8930\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3942 - accuracy: 0.8935\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3950 - accuracy: 0.8920\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3989 - accuracy: 0.8930\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3964 - accuracy: 0.8875\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3862 - accuracy: 0.8946\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3829 - accuracy: 0.8900\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3792 - accuracy: 0.8930\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3876 - accuracy: 0.8930\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4278 - accuracy: 0.8774\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4346 - accuracy: 0.8779\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3975 - accuracy: 0.8925\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3937 - accuracy: 0.8840\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3818 - accuracy: 0.8910\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3901 - accuracy: 0.8895\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "6426b52e-ab3a-4869-bfef-2a7aca6dabb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('bmh')\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "1d4ddb4f-e809-41de-9c34-752980a7347a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 609ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "im feeling really bananas right now of alone on my advice and all singing none of us would scars scars life at park deal scars boomaboomerang life deal scars life smart smart deal deal smart smart deal deal smart deal evening deal breeze scars life smart smart smart deal evening deal rotten advice eyes found boomaboomerang ride over as is life life deal standing life world life smart smart deal deal long smart deal deal smart deal park rotten life rotten life world park life smart bedumbedumdum deal smart smart deal deal smart deal park smart deal deal park smart deal park park smart deal park\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling really bananas right now\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}