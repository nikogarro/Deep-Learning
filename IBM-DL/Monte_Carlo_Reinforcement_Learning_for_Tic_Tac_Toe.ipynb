{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMDeveloperSkillsNetworkGPXX0HAUEN1421-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Monte Carlo Reinforcement Learning for TicTacToe**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is an interesting and challenging way to apply machine learning concepts to a well-known game. You will explore the basics of Reinforcement Learning and the Monte Carlo Method. You will learn how to train your own agent and create a highly-skilled AI player. You will also learn how to work with Open Ai gym environments. Additionally, you will be able to play TicTacToe with our trained agent and environment and see an example of a TicTacToe game with a graphical user interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries a</a></li>\n",
    "            <li><a href=\"#Installing-Custom-Gym-Environment\">Installing Custom Gym Environment a</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Reinforcement-learning\">Reinforcement learning</a></li>\n",
    "    <li><a href=\"#Work-with-Custom-OpenAI-Gym-Environment\">Work with Custom OpenAI Gym Environment</a></li>\n",
    "    <li><a href=\"#Monte-Carlo-Method\">Monte Carlo Method</a></li>\n",
    "    <li><a href=\"#Train-Agent---X\">Train Agent - X</a></li>\n",
    "    <li><a href=\"#Train-Agent---O\">Train Agent - O</a></li>\n",
    "</ol>\n",
    "\n",
    "<a href=\"#Exercises\">Exercises</a>\n",
    "<ol>\n",
    "    <li><a href=\"#Exercise-1---Training-Modifications.\">Exercise 1 - Training Modifications</a></li>\n",
    "    <li><a href=\"#Exercise-2---Test-your-agent.\">Exercise 2 - Test your agent.</a></li>\n",
    "    <li><a href=\"#Exercise-3---Check-the-results.\">Exercise 3 - Check the results.</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "*   Work with an OpenAI Gym environment and custom TicTacToe environment\n",
    "*   Explain what Reinforcement Learning is\n",
    "*   Explain what Monte Carlo Method is\n",
    "*   Create an agent that uses Monte Carlo Method to play TicTacToe\n",
    "*   Train and Test the agents using the TicTacToe environment\n",
    "*   Play some games against the trained agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`gym`](https://www.gymlibrary.dev/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMDeveloperSkillsNetworkGPXX0HAUEN1421-2022-01-01) for OpenAi gym framework \n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
    "*   [`random`](https://docs.python.org/3/library/random.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMDeveloperSkillsNetworkGPXX0HAUEN1421-2022-01-01) to generate random numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "All libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented. If you run this notebook on a different environment, e.g. your desktop, you may need to uncomment and install certain libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.20.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from gym==0.20.0) (1.21.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from gym==0.20.0) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym==0.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Custom Gym Environment\n",
    "\n",
    "First, let's download the environment files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-16 20:00:04--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/playing-tictactoe-with-reinforcement-learning-and-openai-gym/gym-tictactoe.zip\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10119 (9.9K) [application/zip]\n",
      "Saving to: ‘gym-tictactoe.zip.1’\n",
      "\n",
      "gym-tictactoe.zip.1 100%[===================>]   9.88K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-02-16 20:00:04 (24.4 MB/s) - ‘gym-tictactoe.zip.1’ saved [10119/10119]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/playing-tictactoe-with-reinforcement-learning-and-openai-gym/gym-tictactoe.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's unzip the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  gym-tictactoe.zip\n",
      "  inflating: gym-tictactoe/setup.py  \n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/PKG-INFO  \n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/SOURCES.txt  \n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/requires.txt  \n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/top_level.txt  \n",
      "  inflating: gym-tictactoe/gym_tictactoe.egg-info/dependency_links.txt  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/__init__.py  \n",
      "  inflating: gym-tictactoe/.ipynb_checkpoints/setup-checkpoint.py  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/__pycache__/__init__.cpython-39.pyc  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/.ipynb_checkpoints/__init__-checkpoint.py  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/__init__.py  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/tictactoe_env.py  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/__pycache__/__init__.cpython-39.pyc  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/__pycache__/tictactoe_env.cpython-39.pyc  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/.ipynb_checkpoints/__init__-checkpoint.py  \n",
      "  inflating: gym-tictactoe/gym_tictactoe/envs/.ipynb_checkpoints/tictactoe_env-checkpoint.py  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o gym-tictactoe.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the environment we will use `pip install` on the folder with the `setup.py` file in it which is `gym-tictactoe`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///resources/labs/IBMDeveloperSkillsNetwork-GPXX0HAUEN/labs/gym-tictactoe\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gym in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from gym-tictactoe==0.0.1) (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from gym->gym-tictactoe==0.0.1) (1.21.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from gym->gym-tictactoe==0.0.1) (2.2.0)\n",
      "Installing collected packages: gym-tictactoe\n",
      "  Attempting uninstall: gym-tictactoe\n",
      "    Found existing installation: gym-tictactoe 0.0.1\n",
      "    Uninstalling gym-tictactoe-0.0.1:\n",
      "      Successfully uninstalled gym-tictactoe-0.0.1\n",
      "  Running setup.py develop for gym-tictactoe\n",
      "Successfully installed gym-tictactoe-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -e gym-tictactoe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please restart the kernel** using the kernel tab at the top left of the jupyter lab environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import the custom environment and other libraries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gym is a library that will allow us to initialize and work with the TicTacToe environment\n",
    "import gym\n",
    "\n",
    "# Random allows us to make random choices when interacting with the environment\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Custom tictactoe environment\n",
    "import gym_tictactoe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with our project !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, **Reinforcement Learning** is just another machine learning method that is based on rewarding desired actions or outputs and punishing undesired ones. Reinforcement learning models, like people, choose which action to take based on the expected return of each action. To train the model, you must provide input that includes information about the current situation and possible actions, then reward the model based on its output. Reinforcement learning models learn to perform a task through repeated trial-and-error interactions with a dynamic environment, without any human intervention.\n",
    " \n",
    "### Basic Terminology \n",
    "\n",
    "* **Agent**: is your reinforcement learning model, it's a decision maker and learner, \n",
    "\n",
    "* **Environment**: is a world around your agent, agent learns and acts inside of it. The environment takes the action provided by the agent and returns the next\n",
    "**state** and the **reward**.\n",
    "\n",
    "* **State**: is a complete description of the state of the environment. \n",
    "\n",
    "* **Action**: is a way agent interacts with the environment, the moves that your agent can make. **Aciton Space** is the set of all possible actions. \n",
    "\n",
    "* **Reward**: is a feedback from the environment, it can be negative or positive and impacts the agent and serves as an indication to an agent of what you want it to achieve. Rewards are generally unknown and agents learn how to correctly estimate the reward.\n",
    "\n",
    "* **Policy**: is a rule used by an agent to decide what actions to take, given the specific state. It works as a map from state to some action or a set of probabilities for each action in the action space. \n",
    "\n",
    "\n",
    "* **Value Function**: is the function that returns the expected total reward your agent can get from following the specific policy. The agent uses this value function to make decisions and learns by updating the expected reward values of the parameters of this function. In this lab we will be using state-action value function, so our function $Q(s,a)$ will take state-action pair and will return an estimated reward for taking action $a$ from state $s$. \n",
    "\n",
    "### Reinforcement Learning Process\n",
    "1. Agent plays a number of games\n",
    "2. In every game, agent chooses an **Action** form the action space by using **Policy** and **Value Function**\n",
    "3. **Action** impacts the environment and the **Reward** and the new **State** is returned to the agent.\n",
    "4. Agent keeps track of what reward it received after choosing a certain action from a certain set. \n",
    "5. After completing the game, the agent updates the estimated reward for each state and action by using the actual rewards values received while playing the game. \n",
    "6. The whole process repeats again.\n",
    "\n",
    "\n",
    "![RL Interaction Diagram](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/playing-tictactoe-with-reinforcement-learning-and-openai-gym/images/rl_interaction.png)\n",
    "\n",
    "\n",
    "Famous RL models that play Chess, Go or Atari Games on superhuman levels, are all based on the aforementioned principles and concepts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about a TicTacToe related example for each one of them, what an agent would be, what action space does this agent have, how are rewards calculated. In the next section we will implement and explore all of them. At first, these definition may look confusing but don't be scared, we will review and practice them more through out the lab: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with Custom OpenAI Gym Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym is a python library for developing environments that can be used to train reinforcement learning algorithms. An environment can be thought of as the situation or game you are trying to replicate/model which in our case is TicTacToe. We use Gym because it standardizes the structure of the environments so communication between the player and the environment is easy and the same across Gym environments. A player/agent/robot then interacts with the environment by taking actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating an instance of the **Environment** we installed. We can do this using the `gym.make(environment_id)` function where the `environment_id` is the id of the environment located in the `gym-tictactoe/gym_tictactoe/__init__.py` file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('TicTacToe-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have to reset the environment to start working with it. This function resets the game environment to what it was when you started the game:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the actual **Environment** is just a board. let's see how it looks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition `hash` fucntion returns a string representation of our environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---------'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.hash()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the **action space** of the environment. It should be the set of all empty places on the board:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.available_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make some action and see what our environment returns: `step(self, action, player)`\n",
    "\n",
    "When we take a step in our environment we provide the player taking the action which is a string \"X\" or \"O\", so each time-advancing decision is a **step**. Actions in our environment are integers representing the position on the board you would like the player to act on. 0 is the top left of the board and 8 is the bottom right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state, reward, done, info = env.step(0, \"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`new_state` is the hash of the state that the environment is in after the action as you can see the X we put in position 0 is there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X--------'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reward** is a tuple representing the reward each agent receives after the action is performed. The first value is the reward X receives and the second value is the reward O receives. We will use this reward to learn what states are good and bad.\n",
    "\n",
    "*   If X wins the reward is (10, -10)\n",
    "*   If O wins the reward is (-10, 10)\n",
    "*   If the game is a tie or not done yet the reward is (0, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`done` is a boolean used to keep track of whether the game is done or not so we know when to stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`info` is not used in this environment, but can contain information like \"X Wins\" or \"O Loses\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will introduce a bit more terminology, **Episode** is an agent-environment interactions from initial to final states, so it's one game that agent plays. In addition, our agents are operating in a discrete-time game. It's easy to see that each Episode consists of a series of steps. \n",
    "\n",
    "Now let's observe one episode of the game progresses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', 'X']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['-', '-', 'O']\n",
      "['-', '-', 'X']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['X', '-', 'O']\n",
      "['-', '-', 'X']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['X', '-', 'O']\n",
      "['-', 'O', 'X']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['X', '-', 'O']\n",
      "['-', 'O', 'X']\n",
      "['X', '-', '-']\n",
      "Board\n",
      "['X', '-', 'O']\n",
      "['O', 'O', 'X']\n",
      "['X', '-', '-']\n",
      "Board\n",
      "['X', '-', 'O']\n",
      "['O', 'O', 'X']\n",
      "['X', '-', 'X']\n",
      "Board\n",
      "['X', '-', 'O']\n",
      "['O', 'O', 'X']\n",
      "['X', 'O', 'X']\n",
      "Board\n",
      "['X', 'X', 'O']\n",
      "['O', 'O', 'X']\n",
      "['X', 'O', 'X']\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "# variable to keep track of if the game is over\n",
    "done = False\n",
    "# Good practice to reset environment before you play a game to clear any old game\n",
    "env.reset()\n",
    "# Print the initial board\n",
    "env.render()\n",
    "# Want to keep playing untill game is over\n",
    "while not done:\n",
    "    # Make a random action from the list of available actions for X\n",
    "    new_state, reward, done, info = env.step(random.choice(env.available_actions()), \"X\")\n",
    "    # Print board after X action\n",
    "    env.render()\n",
    "    # If the game is done on X action we dont want O to make an action\n",
    "    if not done:\n",
    "        # Make a random action from the list of available actions for O\n",
    "        new_state, reward, done, info = env.step(random.choice(env.available_actions()), \"O\")\n",
    "        # Print board after O action\n",
    "        env.render()\n",
    "\n",
    "# Print the reward after the game is done, reward for X is the first value and O is the second value\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for you to better visualize what and how **policy** works. As a reminder, policy is just a map that maps each state to some action. In the example below, we are playing TicTacToe and we are going second. After the first player places their X, we can see how our policy will tell us to react given the state.\n",
    "\n",
    "![Policy Table](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/playing-tictactoe-with-reinforcement-learning-and-openai-gym/images/policy.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was mentioned earlier, the **state-action value function** is a mathematical function that takes in a particular state of the game board and the action taken by an agent in that state, and returns the expected reward of that action. For example, if an agent takes the action of placing their \"O\" on the middle square of a game board, where there is already one \"X\" in the right corner, the state action value function would take in this state and action and return the expected value for that move (which is $-0.17$ in our diagram below). Here are some visualized examples of state action pairs and what expected return may be returned by state-action value function (represented as $Q(s,a)$). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-GPXX0HAUEN/Screenshot%202022-12-11%20at%2011.37.00%20PM.png\" width=\"100%\" alt=\"iris image\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy Policy\n",
    "Let's check what the difference **Epsilon-Greedy Policy** means. **Epsilon**, is just some constant, ($0 \\leq \\epsilon \\leq 1$), and it will define some probability. **Greedy**, is a concept in computer science where a greedy algorithm is the one that makes the locally optimal choice at each stage. In our case, greedy policy implies that it will choose an action with the biggest estimated return. \n",
    "\n",
    "For now assume that $Q(s,a)$ is our value function, it will return an **estimated** reward based on the given state and action and let $A$ be the action space. Then our policy can be simply defined: \n",
    "\n",
    "\n",
    "$$\n",
    "\\pi(s) =\n",
    "\\begin{cases}\n",
    "a = max_{a^* \\in A}Q(s,a^*)) & \\text{with probability 1-}  \\epsilon \\\\\\\\\\\\\n",
    "\\text{some }a \\in A & \\text{with probability } \\epsilon \\\\\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Let's define a new python function that will follow the epsilon probability and return an action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(action,epsilon=0.1,avb_actions=[]):\n",
    "    ''' \n",
    "    This function takes the best estimated action, eplsilon, and action space \n",
    "    and returns some action. \n",
    "    '''\n",
    "    # generate a random number from 0 to 1.\n",
    "    number = np.random.rand(1)\n",
    "    # if number is smaller than 1-epsilon then return best estimated action\n",
    "    if number < 1 - epsilon:\n",
    "        return action\n",
    "    # if number is bigger or equals to 1-epsilon then return some random action from the action space\n",
    "    else:\n",
    "        action = random.choice(avb_actions)\n",
    "        return action "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may ask why wouldn't we always use the best action, the action with the the best estimated reward, what's the point of this epsilon constant. For it we will have to learn about 2 more concepts: \n",
    " \n",
    "* **Exploration** happens when the agent takes the random action to explore more opportunities, gather more information about possible actions and the environment.\n",
    "* **Exploitation** happens when the agent makes the best decision given current information, it uses the best estimated action to maximize the reward. \n",
    "\n",
    "**Epsilon** defines the trade-off between Exploration and Exploitation.  We need it because the best long-term strategy may involve short-term sacrifices and in most of the cases, agents must explore the environment and gather enough information to make the best overall decisions. It may save our agent from doing decisions that work instead of finding the best actions. \n",
    "\n",
    "Let's test our random_action function: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0\n",
      "a' 0\n",
      "a 1\n",
      "a' 4\n",
      "a 2\n",
      "a' 2\n",
      "a 3\n",
      "a' 3\n"
     ]
    }
   ],
   "source": [
    "test = [0, 1, 2, 3]\n",
    "for t in test:\n",
    "    print(\"a\", t)\n",
    "    print(\"a'\", random_action(t, 0.05, [1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how Monte Carlo Works, following part explains the learning algorithm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Method \n",
    "#### Let's talk about the heart of our algorithm, the Value function that we will be using and how it estimates the reward for each action given the state. \n",
    "\n",
    "Monte Carlo Method was invented by Stanislaw Ulman in 1940s, while trying to calculate the probability of a successful Canfield solitaire (He was sick and had nothing better to do). Ulman randomly lay the cards out and simply calculated the number of successful plays. We will apply the same approach to create our value function. The basic principle of Monte Carlo method can be summarized in 4 steps: \n",
    "\n",
    "1. Define the Domain of Possible inputs \n",
    "2. Generate inputs randomly from a probability distribution over the domain\n",
    "3. Perform a deterministic computation on the inputs\n",
    "4. Average the results\n",
    "\n",
    "## Math Behind It \n",
    "It's ok if you don't understand all of the math and stats details. Try focusing on understanding the general structure and principles of this algorithm. Before we can see it in action let's define a few things. Review that **Episode** is a set of agent-environment interactions from initial to final states which constists of steps in a discrete-time game. \n",
    "\n",
    "Monte Carlo reinforcement learning learns from **episodes of experience**, it functions by setting the value function equal to the empirical mean return.\n",
    "Let's assume that we have some initialized policy $\\pi$ that our agent follows. Then let's play a game once and gain the following episode: \n",
    "\n",
    "$$(S_1, A_1, R_1)\\rightarrow(S_2, A_2, R_2)\\rightarrow  (S_3, A_3, R_3)\\rightarrow ...\\rightarrow  (S_n) \\sim \\pi$$\n",
    "\n",
    "Now let's look at an total expected reward of taking an Action $A_t$ in the state $S_t$, where t is some **time step**. \n",
    "\n",
    "* At **time step** $t=0$ (the first time step), the environment (including the agent) is in some state $S_t = S_0$ (the initial state), takes an action $A_t = A_0$ (the first action in the game) and receives a reward $R_t = R_0$ and the environment (including the agent) moves to a next state $S_{0+1} = S_{1}$\n",
    "\n",
    "Let's define a function $G$, which will just give us the expected total discounted reward at each time step: \n",
    "$$G(t) = R_t +\\gamma R_{t+1}+\\gamma^2 R_{t+2} + ...+ \\gamma^{k}R_{t+k}$$\n",
    "\n",
    "Discount factor $\\gamma \\in \\left[0,1\\right]$ is an important constant. We add the initial return $R_1$ as it is, without modifying the value, then to get the total reward we are adding $R_{t+1}$ but note that the value is multiplied by $0\\leq \\gamma \\leq 1$, so $R_{t+1}$ is only partially added to $R_1$, $R_{t+2}$ is multiplied by $\\gamma^2$, $R_{t+3}$ is multiplied by $\\gamma^3$ and so on. Gamma determines how much the reinforcement learning agent cares about rewards in the distant future relative to those in the immediate future. Note that if $\\gamma=0$ then total expected return will be defined just by initial reward, so agent will only learn and care about actions that produce an immediate reward. \n",
    "\n",
    "Now we can define our action-value function $Q_{\\pi}(S, A)$ for some state $S$ and action $A$ as: \n",
    "$$Q_{\\pi}(S, A) = E_{\\pi}\\left[G(t)|S_t = S, A_t = A \\right ]$$\n",
    "\n",
    "So value function returns the expected value of a total discounted reward $G(t)$ for the time step $t$ at which $S_t = S$ and $A_t = A$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after completing a series of episodes in the game, how can we adjust the expected values, or a bigger question is how's the learning process itself happening in Monte Carlo Method. For it we will use the concept of **Incremental means**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>CLICK here for a detailed explanation of Incremental Means </summary>\n",
    "\n",
    "**Incremental means**, is just the average of values that's computed incrementally. Let $x_1, x_2,..., x_n$ be the set of values, Let $\\mu_1, \\mu_2, ... , \\mu_{n-1}$ be a sequence of means, where $\\mu_1 = x_1$, $\\mu_2 = \\dfrac{x_1+x_2}{2}$ and $\\mu_3 = \\dfrac{x_1+x_2+x_3}{3}$ and so on. Let's see how the mean is defined incrementally:\n",
    "\n",
    "\\begin{align*} \n",
    "\\mu_n &= \\frac{1}{n}\\sum_{i = 1}^{n}x_i\\\\\\\\  \n",
    "&= \\frac{1}{n} (x_n +  \\sum_{i = 1}^{n-1}x_i)\\\\\n",
    "&= \\frac{1}{n}(x_n +  (n-1)\\mu_{n-1})\\\\\n",
    "&= \\mu_{n-1} + \\frac{1}{n}(x_n - \\mu_{n-1})\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put everything together to describe the Monte Calro Method Learning Process. Let's have an episode: \n",
    "$$(S_1, A_1, R_1)\\rightarrow(S_2, A_2, R_2)\\rightarrow  (S_3, A_3, R_3)\\rightarrow ...\\rightarrow  (S_n) \\sim \\pi$$\n",
    "\n",
    "For each (state, action) pair we will keep track of the number of times this (state, action) was visited, let's define function $N(s_t,a_t)$, then every time we visit (state, action) we will update the visits counter and then adjust the running mean:\n",
    "\\begin{align*}\n",
    "N(S_t, A_t)&+=1\\\\\\\\\n",
    "Q(S_t, A_t)&+= \\frac{1}{N(S_t, A_t)}(G(t) - Q(S_t, A_t))\n",
    "\\end{align*}\n",
    "\n",
    "**Controls**: after the $Q$ function is updated (after each episode), we update the policy as well. As mentioned earlier, we update it by simply mapping each state to some action with the highest newly computed expected return. In the next episode the update policy is used to choose an action, and so on. \n",
    "\n",
    "So now we know how to update the action-value function and how to use it in combination with out policy to maximize the rewards, it can be summarized as: \n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IND-GPXX0O9IEN/Screenshot%202022-11-08%20at%201.27.51%20PM.png\" width=\"50%\" alt=\"iris image\">\n",
    "\n",
    "Monte Carlo algorithm/method is a type of **model-free** reinforcement learning, since the agent does not use predictions of the environment response, so it's not trying to create a statistical **model** of the environment.\n",
    "\n",
    "Let's implement it and try to train our model: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent - X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note: \n",
    "* We will be training $Agent_X$, so the one who plays as \"X\" on the board.\n",
    "* We will be training our agent against a random oponent (so oponent, $Agent_O$, will make random actions).\n",
    "* Note that $Reward$ is updated each time either $Agent_X$ or $Agent_O$ takes an action, since the game may end and reward may be received after $Agent_X$ or $Agent_O$ action. The state is updated only after $Agent_O$ takes an action, as it is the state from which $Agent_X$ takes an action. Let's look at the diagram of the last time step in the episode: \n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-GPXX0HAUEN/Screenshot%202023-01-19%20at%203.08.53%20PM.png\" width=\"50%\" alt=\"iris image\">\n",
    "\n",
    "As a result, $(State:-XOXXOOOX , Action:1, Reward: (10,-10))$ is recorded in the episode. Let's check the code implementation: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_X(environment, N_episodes=10000, epsilon=0.1, discount_factor=1):\n",
    "    \"\"\"\n",
    "    This function determines the optimal policy usistate=environment.reset() ng the epsilon greedy method. \n",
    "    The input parameters are the discount factor, the number of episodes, epsilon value and the open AI gym objects.\n",
    "    The class also Specifies if first-visit and every-visit method. The output is the policy,value function and action function.\n",
    "    Returns:\n",
    "    policy\n",
    "    Q funciton\n",
    "    Args:\n",
    "    environment:AI gym balckjack envorment object \n",
    "    epsilon: the exploration factor\n",
    "    N_episodes:number of episodes \n",
    "    discount_factor:discount factor\n",
    "    \"\"\"\n",
    "    # dictionary of estimated action function for FrozenLake\n",
    "    Q = {}\n",
    "\n",
    "    # number of visits to the action function \n",
    "    NumberVisits = {}\n",
    "\n",
    "    # dictionary  for policy \n",
    "    policy = {}\n",
    "\n",
    "    number_actions = 9\n",
    "\n",
    "    for i in range(N_episodes):\n",
    "\n",
    "        # list that stores each state and reward for each episode     \n",
    "        episode = []\n",
    "        # reset the  environment for the next episode and find first state  \n",
    "        environment.reset() \n",
    "        state = environment.hash()\n",
    "\n",
    "        # flag for end of episodes  \n",
    "        done = False\n",
    "\n",
    "        # check if a policy for the state exists  \n",
    "        if state in policy.keys():\n",
    "            # obtain action from policy \n",
    "            action = policy[state]\n",
    "            action = random_action(action, epsilon, environment.available_actions())\n",
    "\n",
    "        else:\n",
    "            # if no policy for the state exists  select a random  action  \n",
    "            action = random.choice(environment.available_actions())\n",
    "\n",
    "        # take action and find next state, reward\n",
    "        (state_1, reward, done, info) = environment.step(action, \"X\")\n",
    "        # append first state, reward and action\n",
    "        episode.append({'state': state, 'reward': reward[0], 'action': action})\n",
    "\n",
    "        # enumerate for each episode \n",
    "        while not(done):\n",
    "            # Agent O should make a random action.\n",
    "            (state, reward, done, info) = environment.step(random.choice(environment.available_actions()), \"O\")\n",
    "\n",
    "            # Check if the episode is done (True) \n",
    "            if not done:\n",
    "                # Check if a policy for the state exists \n",
    "                if state in policy.keys():\n",
    "                    # obtain action from policy \n",
    "                    action = policy[state]\n",
    "                    action = random_action(action,epsilon,environment.available_actions())\n",
    "\n",
    "                else:\n",
    "                    # if no policy for the state exists  select a random  action  \n",
    "                    action = random.choice(environment.available_actions())\n",
    "                # take action and find next state, reward\n",
    "                (state_1, reward, done, info) = environment.step(action, \"X\")\n",
    "\n",
    "            # add state reward and action to list \n",
    "            episode.append({'state': state, 'reward': reward[0], 'action': action})\n",
    "\n",
    "        # reverse list as the return is calculated from the last state\n",
    "        episode.reverse()\n",
    "\n",
    "        # determine the return\n",
    "        G = 0\n",
    "        for t, step in enumerate(episode):\n",
    "\n",
    "            G = discount_factor * G + step['reward']\n",
    "\n",
    "            # increment counter for action \n",
    "            if (step['state'], step['action']) in NumberVisits.keys():\n",
    "                # obtain action from policy\n",
    "                NumberVisits[step['state'], step['action']] += 1\n",
    "\n",
    "            else:\n",
    "                # if no policy for the state exists  select a random  action  \n",
    "                NumberVisits[step['state'], step['action']] = 1\n",
    "\n",
    "            # if the action function value  does not exist, create an array  to store them \n",
    "            if not step['state'] in Q.keys():\n",
    "                Q[step['state']] = np.zeros((number_actions))\n",
    "\n",
    "            # calculate mean of action function Q Value functions using the  recursive definition of mean \n",
    "            Q[step['state']][step['action']] = Q[step['state']][step['action']] + (NumberVisits[step['state'], step['action']]**-1) * (G - Q[step['state']][step['action']])\n",
    "\n",
    "            # setting the value of impossible statee-action pairs to -1000 (not the most elegant but the most efficient way in our simple case)\n",
    "            for i, x in enumerate(step['state']):\n",
    "                if x != \"-\":\n",
    "                    Q[step['state']][i] = -1000\n",
    "\n",
    "            # update the policy to select the action fuciton argment with the largest value randomly select a different action \n",
    "            policy[step['state']] = np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it and see the results: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_X, Q = monte_carlo_X(env, N_episodes=100000, epsilon=0.1, discount_factor=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results of our training and analyze how our agent performs, for the following code the actions of X player will be dictated only by our policy, let's see the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.69999999999999%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "test = 1000\n",
    "for i in range(test):\n",
    "    # variable to keep track of if the game is over\n",
    "    done = False\n",
    "    # Good practice to reset environment before you play a game to clear any old game\n",
    "    env.reset()\n",
    "    # Want to keep playing untill game is over\n",
    "    new_state = env.hash()\n",
    "    while not done:\n",
    "        # Make an action from policy for X\n",
    "        new_state, reward, done, info = env.step(policy_X[new_state], \"X\")\n",
    "\n",
    "        # If the game is done on X action we dont want O to make an action\n",
    "        if not done:\n",
    "            # Make a random action from the list of available actions for O\n",
    "            new_state, reward, done, info = env.step(random.choice(env.available_actions()), \"O\")\n",
    "\n",
    "    total += reward[0]\n",
    "\n",
    "# Print the reward after the game is done, reward for X is the first value and O is the second value\n",
    "print(f\"{total/test *10}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our agent performs extraordinary well against random agent. Sure you want to test it yourself, play a few games against $Agent-X$, let's see who wins. Don't forget to make an available action from the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['-', '-', '-']\n",
      "['-', 'X', '-']\n",
      "['-', '-', '-']\n",
      "Available actions: [0, 1, 2, 3, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['O', '-', '-']\n",
      "['-', 'X', '-']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['O', '-', '-']\n",
      "['X', 'X', '-']\n",
      "['-', '-', '-']\n",
      "Available actions: [1, 2, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['O', '-', '-']\n",
      "['X', 'X', 'O']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['O', 'X', '-']\n",
      "['X', 'X', 'O']\n",
      "['-', '-', '-']\n",
      "Available actions: [2, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['O', 'X', '-']\n",
      "['X', 'X', 'O']\n",
      "['-', 'O', '-']\n",
      "Board\n",
      "['O', 'X', 'X']\n",
      "['X', 'X', 'O']\n",
      "['-', 'O', '-']\n",
      "Available actions: [6, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['O', 'X', 'X']\n",
      "['X', 'X', 'O']\n",
      "['O', 'O', '-']\n",
      "Board\n",
      "['O', 'X', 'X']\n",
      "['X', 'X', 'O']\n",
      "['O', 'O', 'X']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variable to keep track of if the game is over\n",
    "done = False\n",
    "# Good practice to reset environment before you play a game to clear any old game\n",
    "env.reset()\n",
    "# Print the initial board\n",
    "env.render()\n",
    "new_state = env.hash()\n",
    "# Want to keep playing untill game is over\n",
    "while not done:\n",
    "    # Make a random action from the list of available actions for X\n",
    "    new_state, reward, done, info = env.step(policy_X[new_state], \"X\")\n",
    "    # Print board after X action\n",
    "    env.render()\n",
    "    # If the game is done on X action we dont want O to make an action\n",
    "    if not done:\n",
    "        # Make an action from the list of available actions for O\n",
    "        print(f'Available actions: {env.available_actions()}')\n",
    "        new_state, reward, done, info = env.step(int(input()), \"O\")\n",
    "        # Print board after O action\n",
    "        env.render()\n",
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad player after all as you could notice, let's train our $Agent_O$ in a similar manner, we will have to modify the training function a bit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent - O\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again let's review a few things: \n",
    "* We will be training $Agent-O$, so the one who plays as \"O\" on the board.\n",
    "* We will be training our agent against a random oponent (so oponent, $Agent-X$, will make random actions).\n",
    "* Note that $Reward$ is updated each time either $Agent_X$ or $Agent_O$ takes an action, since the game may end and reward may be received after $Agent_X$ or $Agent_O$ action. The state is updated only after $Agent_X$ takes an action since it's the sate from which $Agent_X$ takes an action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_O(environment, N_episodes=10000, epsilon=0.1, discount_factor=1):\n",
    "    \"\"\"\n",
    "    This function determines the optimal policy usistate=environment.reset() ng the epsilon greedy method.\n",
    "    The input parameters are the discount factor, the number of episodes, epsilon value and the open AI gym objects.\n",
    "    The class also Specifies if first-visit and every-visit method.\n",
    "    The output is the policy,value function and action function.\n",
    "    Returns:  \n",
    "    policy \n",
    "    Q funciton\n",
    "    \n",
    "    Args:\n",
    "    environment:AI gym balckjack envorment object \n",
    "    epsilon: the exploration factor\n",
    "    N_episodes:number of episodes \n",
    "    discount_factor:discount factor\n",
    "    \"\"\"\n",
    "    # dictionary of estimated action function for FrozenLake\n",
    "    Q = {}\n",
    "\n",
    "    # number of visits to the action function \n",
    "    NumberVisits = {}\n",
    "\n",
    "    # dictionary  for policy \n",
    "    policy = {}\n",
    "\n",
    "    # number of possible actions\n",
    "    number_actions = 9\n",
    "\n",
    "    for i in range(N_episodes):\n",
    "\n",
    "        # list that stores each state and reward for each episode     \n",
    "        episode = []\n",
    "        # reset the  environment for the next episode and find first state  \n",
    "        environment.reset() \n",
    "        state = environment.hash()\n",
    "\n",
    "        # flag for end of episodes  \n",
    "        done = False\n",
    "\n",
    "        # enumerate for each episode \n",
    "        while not(done):\n",
    "            # Agent X should make a random action\n",
    "            (state_1, reward, done, info) = environment.step(random.choice(environment.available_actions()), \"X\")\n",
    "\n",
    "            # Check if the episode is done (True) \n",
    "            if not done: \n",
    "                if state_1 in policy.keys():\n",
    "                    # obtain action from policy \n",
    "                    action = policy[state_1]\n",
    "                    action = random_action(action, epsilon, environment.available_actions())\n",
    "\n",
    "                else:\n",
    "                    # if no policy for the state exists  select a random  action  \n",
    "                    action = random.choice(environment.available_actions())\n",
    "                # Agent O should make a given aciton.\n",
    "                (state, reward, done, info) = environment.step(action, \"O\")\n",
    "\n",
    "\n",
    "            # add state reward and action to list \n",
    "            episode.append({'state': state_1, 'reward': reward[1], 'action': action})\n",
    "\n",
    "        # reverse list as the return is calculated from the last state\n",
    "        episode.reverse()\n",
    "\n",
    "        # determine the return\n",
    "        G = 0\n",
    "        for t, step in enumerate(episode):\n",
    "\n",
    "            G = discount_factor * G + step['reward']\n",
    "\n",
    "            # increment counter for action \n",
    "            if (step['state'],step['action']) in NumberVisits.keys():\n",
    "                # obtain action from policy \n",
    "                NumberVisits[step['state'], step['action']] += 1\n",
    "            else:\n",
    "                # if no policy for the state exists  select a random  action  \n",
    "                NumberVisits[step['state'], step['action']] = 1\n",
    "\n",
    "            # if the action function value  does not exist, create an array  to store them \n",
    "            if not step['state'] in Q.keys():\n",
    "                Q[step['state']] = np.zeros((number_actions))\n",
    "\n",
    "            # calculate mean of action function Q Value functions using the  recursive definition of mean \n",
    "            Q[step['state']][step['action']] = Q[step['state']][step['action']] + (NumberVisits[step['state'], step['action']]**-1) * (G - Q[step['state']][step['action']])\n",
    "\n",
    "            # setting the value of impossible state-action pairs to -1000 (not the most elegant but the most efficient way in our simple case)\n",
    "            for i, x in enumerate(step['state']):\n",
    "                if x != \"-\":\n",
    "                    Q[step['state']][i] = -1000\n",
    "            # update the policy to select the action fuciton argment with the largest value randomly select a different action \n",
    "            policy[step['state']] = np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it as well and see the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_O, Q = monte_carlo_O(env, N_episodes=100000, epsilon=0.1, discount_factor=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we will run it against the random X player: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.5%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "test = 1000\n",
    "for i in range(test):\n",
    "    # variable to keep track of if the game is over\n",
    "    done = False\n",
    "    # Good practice to reset environment before you play a game to clear any old game\n",
    "    env.reset()\n",
    "    # Want to keep playing untill game is over\n",
    "    new_state = env.hash()\n",
    "    while not done:\n",
    "        # Make a random action from the list of available actions for X\n",
    "        new_state, reward, done, info = env.step(random.choice(env.available_actions()), \"X\")\n",
    "\n",
    "        # If the game is done on X action we dont want O to make an action\n",
    "        if not done:\n",
    "            # Make an action from the list of available actions for O\n",
    "            new_state, reward, done, info = env.step(policy_O[new_state], \"O\")\n",
    "\n",
    "    total += reward[1]\n",
    "\n",
    "    # Print the reward after the game is done, reward for X is the first value and O is the second value\n",
    "print(f\"{total/test *10}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we get a perfect score, but since we have a trained $X$ Agent, why don't we try putting one against another: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of games (episodes)\n",
    "def test(episodes):\n",
    "    # counters to keep track of results\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    # loops for a certain number of games (episodes)\n",
    "    for episode in range(episodes):\n",
    "        # stops while loop when game is done\n",
    "        done = False\n",
    "        # resets environment when game is done\n",
    "        env.reset()\n",
    "        new_state = env.hash()\n",
    "        # X agents turn\n",
    "        # performs an action\n",
    "        new_state, reward, done, _ = env.step(policy_X[new_state], \"X\")\n",
    "        while not done:\n",
    "            # O agents turn\n",
    "            # Get the best action\n",
    "            new_state, reward, done, _ = env.step(policy_O[new_state], \"O\")\n",
    "\n",
    "            # if the game ends on O move, we don't want to make an X move\n",
    "            if (not done):\n",
    "                # X agents turn\n",
    "                # performs an action\n",
    "                new_state, reward, done, _ = env.step(policy_X[new_state], \"X\")\n",
    "        # record results when game is done\n",
    "        if (reward == (-10, 10)):\n",
    "            o_win += 1\n",
    "        elif (reward == (10, -10)):\n",
    "            x_win += 1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie += 1\n",
    "    return x_win, o_win, tie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our agents never win against each other only tie, try running the `test` function with more episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Win: 0 Tie: 10000 O Win: 0\n",
      "X Win Rate: 0.0 Tie Rate: 100.0 O Win Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "\n",
    "x_win, o_win, tie = test(episodes)\n",
    "\n",
    "print(\"X Win:\", x_win, \"Tie:\", tie, \"O Win:\", o_win)\n",
    "print(\"X Win Rate:\", x_win/episodes*100, \"Tie Rate:\", tie/episodes*100, \"O Win Rate:\", o_win/episodes*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above should return a perfect Tie Rate of $100\\%$, which makes sense since we are trying to . Now you can also play against $Agent-O$, let's see who is better: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "['-', '-', '-']\n",
      "Available actions: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', '-', '-']\n",
      "['-', 'X', '-']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['-', '-', 'O']\n",
      "['-', 'X', '-']\n",
      "['-', '-', '-']\n",
      "Available actions: [0, 1, 3, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', 'X', 'O']\n",
      "['-', 'X', '-']\n",
      "['-', '-', '-']\n",
      "Board\n",
      "['-', 'X', 'O']\n",
      "['-', 'X', 'O']\n",
      "['-', '-', '-']\n",
      "Available actions: [0, 3, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board\n",
      "['-', 'X', 'O']\n",
      "['-', 'X', 'O']\n",
      "['-', 'X', '-']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, -10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variable to keep track of if the game is over\n",
    "done = False\n",
    "# Good practice to reset environment before you play a game to clear any old game\n",
    "env.reset()\n",
    "# Print the initial board\n",
    "env.render()\n",
    "new_state = env.hash()\n",
    "# Want to keep playing untill game is over\n",
    "while not done:\n",
    "    # Make an action fromthe list of available actions for X\n",
    "    print(f'Available actions: {env.available_actions()}')\n",
    "    new_state, reward, done, info = env.step(int(input()), \"X\")\n",
    "    # Print board after X action\n",
    "    env.render()\n",
    "    # If the game is done on X action we dont want O to make an action\n",
    "    if not done:\n",
    "        # Make an action using policy for O\n",
    "        new_state, reward, done, info = env.step(policy_O[new_state], \"O\")\n",
    "        # Print board after O action\n",
    "        env.render()\n",
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Training Modifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to train a reinforcement learning agent, previously we trained our X agent against O agent that plays randomly now let's train an X agent against an O agent that acts according to well defined `policy_O`. Using the train and test functions previously defined create a new train function that trains an X agent against policy oriented O agent.There are many ways to train a reinforcement learning agent, previously we trained our X agent against O agent that plays randomly now let's train an X agent against an O agent that acts accordingly to well defined `policy_O`. Using the train and test functions previously defined create a new train function that trains an X agent against policy oriented O agent. (Change one line of code from TODO to making an action based on the policy_O)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_X(environment, N_episodes=10000, epsilon=0.1, discount_factor=1):\n",
    "    \"\"\"\n",
    "    This function determines the optimal policy usistate=environment.reset() ng the epsilon greedy method. \n",
    "    The input parameters are the discount factor, the number of episodes, epsilon value and the open AI gym objects.\n",
    "    The class also Specifies if first-visit and every-visit method. \n",
    "    The output is the policy,value function and action function.\n",
    "    Returns:  \n",
    "    policy \n",
    "    Q funciton\n",
    "    \n",
    "    Args:\n",
    "    environment:AI gym balckjack envorment object \n",
    "    epsilon: the exploration factor\n",
    "    N_episodes:number of episodes \n",
    "    discount_factor:discount factor\n",
    "    \"\"\"\n",
    "    # dictionary of estimated action function for FrozenLake\n",
    "    Q = {}\n",
    "\n",
    "    # number of visits to the action function \n",
    "    NumberVisits = {}\n",
    "\n",
    "    # dictionary  for policy \n",
    "    policy = {}\n",
    "\n",
    "    number_actions = 9\n",
    "\n",
    "    for i in range(N_episodes):\n",
    "\n",
    "        # list that stores each state and reward for each episode     \n",
    "        episode = []\n",
    "        # reset the  environment for the next episode and find first state  \n",
    "        environment.reset() \n",
    "        state = environment.hash()\n",
    "\n",
    "        # flag for end of episodes  \n",
    "        done = False\n",
    "\n",
    "        # check if a policy for the state exists  \n",
    "        if state in policy.keys():\n",
    "            # obtain action from policy \n",
    "            action = policy[state]\n",
    "            action = random_action(action, epsilon, environment.available_actions())\n",
    "\n",
    "        else:\n",
    "            # if no policy for the state exists  select a random  action  \n",
    "            action = random.choice(environment.available_actions())  \n",
    "\n",
    "        # Agent X should perform a given action\n",
    "        (state_1, reward, done, info) = environment.step(action, \"X\")\n",
    "        # append first state, reward and action\n",
    "        episode.append({'state': state, 'reward': reward[0], 'action': action})\n",
    "\n",
    "        # enumerate for each episode \n",
    "        while not(done):\n",
    "            (state, reward, done, info) = environment.step(policy_O[new_state], \"O\")  #  !!!TODO write the code that makes an O action based on the policy_O (use environment.step())!!!\n",
    "\n",
    "            if not done:\n",
    "                if state in policy.keys():\n",
    "                    # obtain action from policy \n",
    "                    action = policy[state]\n",
    "                    action = random_action(action, epsilon, environment.available_actions())\n",
    "                else:\n",
    "                    # if no policy for the state exists  select a random  action  \n",
    "                    action = random.choice(environment.available_actions())\n",
    "\n",
    "                # take action and find next state, reward and check if the episode is done (True)\n",
    "                (state_1, reward, done, info) = environment.step(action, \"X\")\n",
    "\n",
    "            # add state reward and action to list \n",
    "            episode.append({'state': state, 'reward': reward[0], 'action': action})\n",
    "\n",
    "        # reverse list as the return is calculated from the last state\n",
    "        episode.reverse()\n",
    "\n",
    "        # determine the return\n",
    "        G = 0\n",
    "        for t, step in enumerate(episode):\n",
    "\n",
    "            G = discount_factor*G+step['reward']\n",
    "\n",
    "            # increment counter for action \n",
    "            if (step['state'],step['action']) in NumberVisits.keys():\n",
    "                # obtain action from policy \n",
    "                NumberVisits[step['state'], step['action']] += 1\n",
    "            else:\n",
    "                # if no policy for the state exists  select a random  action  \n",
    "                NumberVisits[step['state'], step['action']] = 1\n",
    "            # if the action function value  does not exist, create an array  to store them \n",
    "            if not step['state'] in Q.keys():\n",
    "                Q[step['state']] = np.zeros((number_actions))\n",
    "\n",
    "            # calculate mean of action function Q Value functions using the  recursive definition of mean \n",
    "            Q[step['state']][step['action']] = Q[step['state']][step['action']] + (NumberVisits[step['state'], step['action']]**-1) * (G-Q[step['state']][step['action']])\n",
    "\n",
    "            # setting the value of impossible statee-action pairs to -1000\n",
    "            for i, x in enumerate(step['state']):\n",
    "                if x != \"-\":\n",
    "                    Q[step['state']][i] = -1000\n",
    "\n",
    "            # update the policy to select the action fuciton argment with the largest value randomly select a different action \n",
    "            policy[step['state']] = np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for a Hint</summary>\n",
    "    \n",
    "```Python   \n",
    "def monte_carlo_X(environment,N_episodes=10000,epsilon=0.1, discount_factor=1):\n",
    "    \"\"\"\n",
    "    This function determines the optimal policy usistate=environment.reset() ng the epsilon greedy method. The input parameters are the discount factor, the number of episodes, epsilon value and the open AI gym objects. The class also Specifies if first-visit and every-visit method. The output is the policy,value function and action function.\n",
    "    Returns:  \n",
    "    policy \n",
    "    Q funciton\n",
    "    \n",
    "    Args:\n",
    "    environment:AI gym balckjack envorment object \n",
    "    epsilon: the exploration factor\n",
    "    N_episodes:number of episodes \n",
    "    discount_factor:discount factor\n",
    "    \"\"\"\n",
    "    #dictionary of estimated action function for FrozenLake\n",
    "    Q={}\n",
    "    \n",
    "    # number of visits to the action function \n",
    "    NumberVisits= {}\n",
    "   \n",
    "    #dictionary  for policy \n",
    "    policy={}\n",
    "    \n",
    "    number_actions = 9\n",
    "    \n",
    "    for i in range(N_episodes):\n",
    "        \n",
    "        #list that stores each state and reward for each episode     \n",
    "        episode=[]\n",
    "        # reset the  environment for the next episode and find first state  \n",
    "        environment.reset() \n",
    "        state = environment.hash()\n",
    "        \n",
    "        #flag for end of episodes  \n",
    "        done=False\n",
    "        \n",
    "         #check if a policy for the state exists  \n",
    "        if state in policy.keys():\n",
    "            #obtain action from policy \n",
    "            action= policy[state]\n",
    "            action= random_action(action,epsilon,environment.available_actions())\n",
    "\n",
    "        else:\n",
    "            #if no policy for the state exists  select a random  action  \n",
    "            action=random.choice(environment.available_actions())  \n",
    "        \n",
    "        #Agent X should perform a given action\n",
    "        (state_1, reward, done, info) = environment.step(action, \"X\")\n",
    "        #append first state, reward and action\n",
    "        episode.append({'state':state, 'reward':reward[0],'action':action})\n",
    "        \n",
    "        #enumerate for each episode \n",
    "        while not(done):\n",
    "            #TODO:\n",
    "            (state, reward, done, info) = environment.step(policy_O[new_state], \"O\")\n",
    "                \n",
    "            if not done:\n",
    "                if state in policy.keys():\n",
    "                    #obtain action from policy \n",
    "                    action=policy[state]\n",
    "                    action= random_action(action,epsilon,environment.available_actions())\n",
    "\n",
    "                else:\n",
    "                    #if no policy for the state exists  select a random  action  \n",
    "                    action=random.choice(environment.available_actions())\n",
    "                    \n",
    "                #take action and find next state, reward and check if the episode is done (True)\n",
    "                (state_1, reward, done, info) = environment.step(action, \"X\")\n",
    "                \n",
    "            #add state reward and action to list \n",
    "            episode.append({'state':state, 'reward':reward[0],'action':action})\n",
    "        \n",
    "        #reverse list as the return is calculated from the last state\n",
    "        episode.reverse()\n",
    "\n",
    "        # determine the return\n",
    "        G=0\n",
    "        for t,step in enumerate(episode):\n",
    "                \n",
    "                G=discount_factor*G+step['reward']\n",
    "                \n",
    "                #increment counter for action \n",
    "                if (step['state'],step['action']) in NumberVisits.keys():\n",
    "                    #obtain action from policy \n",
    "                    NumberVisits[step['state'],step['action']]+=1\n",
    "\n",
    "                else:\n",
    "                    #if no policy for the state exists  select a random  action  \n",
    "                    NumberVisits[step['state'],step['action']]=1\n",
    "        \n",
    "                #if the action function value  does not exist, create an array  to store them \n",
    "                if not step['state'] in Q.keys():\n",
    "                    Q[step['state']]= np.zeros((number_actions))\n",
    "\n",
    "                #calculate mean of action function Q Value functions using the  recursive definition of mean \n",
    "                Q[step['state']][step['action']]=Q[step['state']][step['action']]+(NumberVisits[step['state'],step['action']]**-1)*(G-Q[step['state']][step['action']])\n",
    "                \n",
    "                #setting the value of impossible statee-action pairs to -1000\n",
    "                for i,x in enumerate(step['state']):\n",
    "                    if x != \"-\":\n",
    "                        Q[step['state']][i] = -1000\n",
    "                        \n",
    "                #update the policy to select the action fuciton argment with the largest value randomly select a different action \n",
    "                policy[step['state']]=np.random.choice(np.where(Q[step['state']]==Q[step['state']].max())[0])\n",
    "                        \n",
    "   \n",
    "    return policy, Q\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your agent and find an optimal policy for Agent-X, with 100000 episodes, epsilon of 0.1, discount_factor of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_X, Q = monte_carlo_X(env, N_episodes=100000, epsilon=0.1, discount_factor=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for a Solution</summary>\n",
    "    \n",
    "```Python   \n",
    "policy_X, Q = monte_carlo_X(env,N_episodes=100000,epsilon=0.1, discount_factor=1)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Test your agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a testing function to test the performance of your newly trained Agent X against Agent O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_x(episodes):\n",
    "    # counters to keep track of results\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    # loops for a certain number of games (episodes)\n",
    "    for episode in range(episodes):\n",
    "        # stops while loop when game is done\n",
    "        done = False\n",
    "        # resets environment when game is done\n",
    "        env.reset()\n",
    "        new_state = env.hash()\n",
    "        # X agent performs an action\n",
    "        new_state, reward, done, _ = env.step(policy_X[new_state], \"X\")\n",
    "        while not done:\n",
    "            # O agents turn\n",
    "            new_state, reward, done, _ = env.step(policy_O[new_state], \"O\")\n",
    "            # if the game ends on O move, we don't want to make an X move\n",
    "            if (not done):\n",
    "                # X agents turn\n",
    "                new_state, reward, done, _ = env.step(policy_X[new_state], \"X\")\n",
    "        # record results when game is done\n",
    "        if (reward == (-10, 10)):\n",
    "            o_win += 1\n",
    "        elif (reward == (10, -10)):\n",
    "            x_win += 1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie += 1\n",
    "    return x_win, o_win, tie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> <summary>Click here for a Solution</summary>\n",
    "    \n",
    "```Python  \n",
    "def test_x(episodes):\n",
    "    # counters to keep track of results\n",
    "    x_win = 0\n",
    "    o_win = 0\n",
    "    tie = 0\n",
    "    # loops for a certain number of games (episodes)\n",
    "    for episode in range(episodes):\n",
    "        # stops while loop when game is done\n",
    "        done = False\n",
    "        # resets environment when game is done\n",
    "        env.reset()\n",
    "        new_state = env.hash()\n",
    "        # X agents turn\n",
    "        # performs an action\n",
    "        new_state, reward, done, _ = env.step(policy_X[new_state], \"X\")\n",
    "        while not done:\n",
    "            # O agents turn\n",
    "            # Get the best action\n",
    "            new_state, reward, done, _ = env.step(policy_O[new_state], \"O\")\n",
    "            \n",
    "            # if the game ends on O move, we don't want to make an X move\n",
    "            if (not done):\n",
    "                 # X agents turn\n",
    "                 # performs an action\n",
    "                new_state, reward, done, _ = env.step(policy_X[new_state], \"X\")\n",
    "        # record results when game is done\n",
    "        if (reward == (-10, 10)):\n",
    "            o_win+=1\n",
    "        elif (reward == (10, -10)):\n",
    "            x_win+=1\n",
    "        elif (reward == (0, 0)):\n",
    "            tie+=1\n",
    "    return x_win, o_win, tie\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Check the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple question, finish code below to show the results of your training and testing. Test it with $10000$ episodes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'--O-X----'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1461/3823746767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Win:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tie:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1461/1005828655.py\u001b[0m in \u001b[0;36mtest_x\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;31m# X agents turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# record results when game is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '--O-X----'"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "\n",
    "win, loss, tie = test_x(episodes)\n",
    "\n",
    "print(\"Win:\", win, \"Tie:\", tie, \"Loss:\", loss)\n",
    "print(\"Win Rate:\", win / episodes*100, \"Tie Rate:\", tie/episodes*100, \"Loss Rate:\", loss / episodes*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here for a Solution</summary>\n",
    "    \n",
    "```Python    \n",
    "episodes = 10000\n",
    "\n",
    "win, loss, tie = test_x(episodes)\n",
    "\n",
    "print(\"Win:\", win, \"Tie:\", tie, \"Loss:\", loss)\n",
    "print(\"Win Rate:\", win/episodes*100, \"Tie Rate:\", tie/episodes*100, \"Loss Rate:\", loss/episodes*100)\n",
    "```\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few additional notes: Monte Carlo Algorithm fails in a stochastic environment, especially when the environment is very big and complex. To work with more complex, non deterministic environments we will need a more complex model/algorithm. If you are interested, you can research more into models like Deep Q-Learning, where instead of using a simple Q function, we use a Neural Network that takes a state and approximates the Q-values for each action based on that state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! - You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [ Algorithms and pseudocode: Sutton, Richard S., and Andrew G. Barto. \"Reinforcement learning: An introduction\"](https://www.amazon.ca/Reinforcement-Learning-Introduction-Second-Paperback/dp/B0B95WFGV6/ref=asc_df_B0B95WFGV6/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMDeveloperSkillsNetworkGPXX0HAUEN1421-2022-01-01&tag=googleshopc0c-20&linkCode=df0&hvadid=578919340205&hvpos=&hvnetw=g&hvrand=11011609051689383259&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9000828&hvtargid=pla-1728961664549&psc=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
